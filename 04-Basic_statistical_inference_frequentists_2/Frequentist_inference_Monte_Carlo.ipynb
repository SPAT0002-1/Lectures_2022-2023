{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Statistical Inference: Uncertainty calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content:\n",
    "\n",
    "- II. What is statistical inference ? See [Frequentist_inference_01.ipynb](Frequentist_inference_01.ipynb)\n",
    "    * II.1 Point estimate.  See [Frequentist_inference_01.ipynb](Frequentist_inference_01.ipynb)\n",
    "    * II.2 Confidence interval. See [Frequentist_inference_02.ipynb](Frequentist_inference_02.ipynb)\n",
    "    * II.3 Hypothesis testing See [Frequentist_inference_H_test.ipynb](Frequentist_inference_H_test.ipynb)\n",
    "    * II.4 Uncertainty calculation using Monte-Carlo method:\n",
    "        - II.4.1 [Variable transformation](#II.4.1-Variable-transformation)\n",
    "        - II.4.2 [Error propagation formula](#II.4.2-Error-propagation-formula:)\n",
    "        - II.4.3 [Monte-Carlo-Error-estimate](#II.4.3-Monte-Carlo-Error-estimate:)\n",
    "    * II.5 Bootstrap. See [Frequentist_inference_Bootstrap.ipynb](Frequentist_inference_Bootstrap.ipynb)\n",
    "    \n",
    "- X. [References and supplementary material](#X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to be used in this notebook\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 Monte-Carlo methods and error estimates\n",
    "\n",
    "The random variable you are interested in is often a function of other random variables that are directly measured. A simple example in physics is the measurement of the speed of an object. You may not measure the speed directly but the distance and the time. For each time and distance you measure, you estimate have an associated uncertainty. But what you are interested in is the velocity, i.e. distance / unit of time. How do you propagate the two uncertainties to derive the uncertainty on the speed ? We'll see below how to proceed thanks to the **error propagation formula** (this is a formula you should know and apply !). But we'll also introduce a numerical approach (Monte-Carlo technique) to tackle that question. This is particularly useful if you are in a situation where you cannot easily apply the error propagation formula (you need to calculate partial derivatives on random variable), or when you suspect that this formula is not valid (it is an approximate formula).\n",
    "\n",
    "### II.4.1 Variable transformation\n",
    "\n",
    "Imagine that you have a sample of magnitude measurements of an object ( $m = -2.5 * log(F)$, where $F$ is the flux). How do you estimate the mean and uncertainty on the mean ? You may be tempted to use the standard error on the mean (see [I.7.3](#I.7.3-Standard-error-on-the-mean) ), but you cannot because the magnitude is a derived quantity, the flux being what is effectively measured and for which you measure errors.    \n",
    "\n",
    "As you may know, any function of a random variable is a random variable. Let's consider the \"primal\" random variable $x$ and the function $y = \\phi(x)$ of the latter.    \n",
    "\n",
    "If we know the pdf $p(x)$ (where $x$ is a random variable), then [remember](../03-Basic_statistics_and_proba_concepts/Descriptive_statistics_01.ipynb) that the probability distribution $p(y)$, where $y = \\phi(x)$ (and so $x = \\phi^{-1}(y)$), is:\n",
    "\n",
    "$$\n",
    "p(y) = P'(y) = p\\left[ \\phi^{-1} \\left(y \\right) \\right] \\left| \\frac{{\\rm d}\\,\\phi^{-1}(y) }{{\\rm{d}} y} \\right|\n",
    "$$\n",
    "\n",
    "where $P'(y)$ is the first derivative of the CDF.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where $x$ is drawn from a uniform distribution, but we study $y = -2.5*log(x)$. Then we have $x = \\phi^{-1}(y) = 10^{-0.4*y} $. Since we have a uniform distribution for $x$, we have $p(x) = 1$ if $0 \\leq x \\leq 1$, and therefore $p(y) =  0.4\\, \\ln(10) \\, 10^{-0.4\\,y}$ with $0 \\,< \\,y \\,<\\,\\infty $. \n",
    "\n",
    "Note that we take the absolute value of the first derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a uniform distribution\n",
    "uniform_dist = scipy.stats.uniform(0.0001, 1)\n",
    "x_sample = uniform_dist.rvs(1000)\n",
    "# Calculate the pdf of x to overplot it\n",
    "x = np.linspace(-0.5, 1.5, 1000)\n",
    "Px = uniform_dist.pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "y_sample = -2.5*np.log10(x_sample)\n",
    "# Calculate the pdf of y to overplot it\n",
    "y = -2.5*np.log10(x)\n",
    "Py = 0.4 * np.log(10) * Px * 10**(-0.4*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Uniform and transformed distribution \n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist(x_sample, 20, histtype='stepfilled', fc='blue', density=True)\n",
    "ax.plot(x, Px, '-k')\n",
    "ax.set_xlim(-0.2, 1.2)\n",
    "ax.set_ylim(0, 1.4001)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "ax.text(0.95, 0.95, r'$p_x(x) = {\\rm Uniform}(x)$', va='top', ha='right', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p_x(x)$')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.hist(y_sample, 20, histtype='stepfilled', fc='blue', density=True)\n",
    "ax.plot(y, Py, '-k')\n",
    "#ax.set_xlim(0.85, 2.9)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "ax.text(0.95, 0.95, '$y=-2.5\\,\\log(x)$\\n$p_y(y)=p_x(c\\,10^{-0.4\\,y})$', va='top', ha='right', transform=ax.transAxes)\n",
    "ax.set_xlabel('$y$')\n",
    "ax.set_ylabel('$p_y(y)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.2 Error propagation formula:\n",
    "\n",
    "\n",
    "How do you proceed if you have a function of more than one random variable ? Then, you need to use the *error propagation formula* given below. This is a **very important** formula that you need to know. Knowing the demonstration is not compulsory, but understanding it is insightful and allows you to understand the assumptions entering into the formula.   \n",
    "\n",
    "If we have a function $z = \\phi(x,y)$, where $x $ and $y$ are random variable with sample values $x_i = {x_1, ..., x_n}$ and $y_i = {y_1, ..., y_n}$, then the error on $\\sigma_z$ is given by the **Error propagation formula**:  \n",
    "\n",
    "$$\n",
    "\\sigma_z^2 \\, = \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\sigma_x^2 \\, + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2  \\sigma_y^2  + 2  \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\sigma_{xy}\n",
    "$$\n",
    "\n",
    "*Demonstration:* \n",
    "\n",
    "We can define $z_i$ as $\\phi(x_i, y_i)$ (i.e. $\\phi$ evaluated at $(x_i, y_i)$), and $\\bar{z}$ as $\\phi(\\bar{x}, \\bar{y})$ (i.e. $\\phi$ evaluated at its mean value). \n",
    "\n",
    "The Taylor expansion of $z$ around its average value is:\n",
    "\n",
    "$$\n",
    "z_i = \\phi(\\bar{x}, \\bar{y}) \\, + \\,  \\left(\\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}  \\, (x_i-\\bar{x})+ \\, \\left (\\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\, (y_i-\\bar{y})  ~ + ~{\\rm {higher~order~terms}}\n",
    "$$\n",
    "\n",
    "If the measured values $x_i, y_i$ are close to the average, then, neglecting the higher order terms, we have:   \n",
    "\n",
    "$$\n",
    "z_i - \\bar{z} = \\left(\\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}  \\, (x_i-\\bar{x})+ \\, \\left (\\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\, (y_i-\\bar{y})\n",
    "$$\n",
    "\n",
    "We can then calculate the variance on $\\Phi$ which is:    \n",
    "\n",
    "$$\n",
    "\\sigma_z^2 \\, = \\, \\sum_i^{N} \\, \\frac{(z_i - \\bar{z})^2}{N} \n",
    "$$\n",
    "\n",
    "Hence,    \n",
    "$$\n",
    "\\sigma_z^2 \\, = \\, \\frac{1}{N} \\sum_i^{N} \\, (x_i-\\bar{x})^2 \\, \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\,   +  \\frac{1}{N} \\sum_i^{N} \\, (y_i-\\bar{y})^2 \\, \\left (\\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2 \\,  + \\frac{2}{N}  \\sum_i^{N}   \\, (x_i-\\bar{x}) (y_i-\\bar{y}) \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_z^2 \\, = \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\sigma_x^2 \\, + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2  \\sigma_y^2  + 2  \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\sigma_{xy}\n",
    "$$\n",
    "\n",
    "Such error estimates for non-linear functions (i.e. non linear in their variables) are biased on account of using a truncated series expansion. \n",
    "\n",
    "Note also that even in the simpler case where $z = \\phi(x)$ (simple transformation of variable), there is no guarantee, if $x$ follows a gaussian distribution of width $\\sigma_x$, that the transformed variable $z$ also follows a gaussian distribution of width $\\sigma_z$. \n",
    "\n",
    "**Conclusions:** Although there is no explicit assumption regarding the pdf in the derivation of the \"error propagation\" formula, there is an implicit one which comes from neglecting higher order terms. We'll see hereafter how Monte-Carlo simulations can be used to study the distribution of a transformed variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.3 Monte-Carlo Error estimate:\n",
    "\n",
    "The methods that consists in generating random points drawn from a specific distribution function and use them for various purposes are generically classified among *Monte-Carlo* techniques. \n",
    "\n",
    "By using Monte-Carlo simulations, we can verify the validity of the error propagation formula, but also have a way to derive the mean value and variance of a random variable that results from a combination of several random variables. Indeed, random samples allow us to approximate a pdf by drawing a virtually infinite samples from it. This means that:\n",
    "\n",
    "$$\n",
    "E(f(x)) \\, = \\, \\int\\limits_{-\\infty}^{+\\infty} f(x) h(x) dx \\rightarrow \\frac{1}{N}\\sum\\limits_i^N f(x_i),\n",
    "$$\n",
    "\n",
    "where $x_i$ is a random number drawn from the distribution $h(x)$, which, for the purpose of Monte-carlo simulations, is a uniform distribution ($h(x) = 1/(b-a)$).   \n",
    "If we specialize that formula for the mean, we have:\n",
    "\n",
    "$$\n",
    "\\mu = \\int_{-\\infty}^{+\\infty} x \\,h(x) dx \\rightarrow \\frac{1}{N}\\sum\\limits_i^N x_i,\n",
    "$$\n",
    "\n",
    "and the variance\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 \\, h(x) dx \\rightarrow \\frac{1}{N}\\sum\\limits_i^N (x_i - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "It's basically as if we did our experiment nearly infinitely many times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's first apply the Monte-carlo approach to the simple case of the sum of two random variables ($a$ and $b$) drawn from a gaussian distribution, and compare the result to the Gaussian distribution we derive from std error propagation formula:\n",
    "\n",
    "    * **z = a + b**\n",
    "\n",
    "*Standard error propagation:*\n",
    "\n",
    "$\\sigma_z^2 = \\left(\\frac{\\partial z}{\\partial a}\\right)^2 \\sigma_a^2 + \\left(\\frac{\\partial z}{\\partial b}\\right)^2 \\sigma_b^2$\n",
    "\n",
    "$\\Longrightarrow \\sigma_z^2 =  \\sigma_a^2 + \\sigma_b^2$\n",
    "\n",
    "The comparison between the two methods is given as an **Exercise**. Follow the different steps outlined in the next 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE \n",
    "# Define 2 random variable a drawn from N(mu_a, sigma_a)  and b drawn from N(mu_b, sigma_b)\n",
    "\n",
    "# We give values to mu_a, mu_b, sigma_a, sigma_b\n",
    "\n",
    "\n",
    "# Create an instance a of N(mu_a, sigma_a), and b of N(mu_b, sigma_b) using scipy.stats.norm()\n",
    "\n",
    "\n",
    "# Generate a random sample of 10000 points for each distribution \n",
    "\n",
    "\n",
    "# Our random variable z is the sum of the 2 random variables a and b \n",
    "# => we generate a sample of z which is the sum of sample_a and sample_b\n",
    "\n",
    "\n",
    "# Visualize the distribution of the Monte-Carlo sample you just created  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE - 2nd part\n",
    "# We want to compare the Monte-carlo to error propagation formula expectation \n",
    "# => we create, for comparison, z drawn from N(mu_z, sigma_z) with mu_z and sigma_z given by error propagation\n",
    "\n",
    "\n",
    "# Plot the distribution of your Montecarlo sample and overplot the PDF of the N(mu_z, sigma_z) distribution \n",
    "# with (mu_z, sigma_z) given by the error propagation formula \n",
    "\n",
    "\n",
    "# Calculate the estimate of the mean and std from the error propagation \n",
    "\n",
    "\n",
    "# Print the values of the estimate of the mean and std from the error propagation and those derived with MC\n",
    "# Do they agree ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now try with the product of two random variables:  **z = a * b**\n",
    "\n",
    "Standard Error Propagation:\n",
    "\n",
    "$\\sigma_z^2 = b^2\\sigma_a^2 + a^2\\sigma_b^2 = z^2\\left(\\left(\\frac{\\sigma_a}{a}\\right)^2 + \\left(\\frac{\\sigma_b}{b}\\right)^2\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the 2 distributions for rv_1= a and rv_2 = b ; here we take 2 gaussian parent distributions\n",
    "mu_a, sigma_a = 1., 0.6\n",
    "mu_b, sigma_b = 1., 1.\n",
    "\n",
    "a = scipy.stats.norm(mu_a, sigma_a)\n",
    "b = scipy.stats.norm(mu_b, sigma_b)\n",
    "sample_a = a.rvs(10000)\n",
    "sample_b = b.rvs(10000)\n",
    "# let's build the random variable z = a * b\n",
    "sample_z = sample_a * sample_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build N(mu_z, sigma_z) where sigma_z is calculated from the error propagation formula\n",
    "mu_z = (mu_a * mu_b) \n",
    "sigma_z = mu_z * np.sqrt((sigma_a / mu_a)**2  + (sigma_b / mu_b)**2)  # error propagation formula\n",
    "z = scipy.stats.norm(mu_z, sigma_z)\n",
    "\n",
    "z_hist = plt.hist(sample_z, bins=50, normed=True, color='blue', alpha=0.3, label='MC')\n",
    "x = np.linspace(-5, 40, 100)\n",
    "plt.plot(x, z.pdf(x), color='red', label='Std err')\n",
    "plt.legend()\n",
    "\n",
    "sample_mu_z = np.mean(sample_z)\n",
    "sample_sig_z = np.std(sample_z)\n",
    "\n",
    "print(\"Error from error propagation: z = %1.2f +- %1.2f\" % (mu_z, sigma_z))\n",
    "print(\"MC Error: z = %1.2f +- %1.2f\" % (sample_mu_z, sample_sig_z) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the error on b in the above example, and see how the final distribution behaves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "We have seen (that): \n",
    "\n",
    "- How the distribution of a random variable $X$ is transformed the we consider a function $Y = \\phi(X)$. The change of variable formula is similar to what we use for non random variable.\n",
    "- The **standard error propagation formula** allows us to calculate the uncertainty on a random variable $Z = \\phi(X, Y)$.  We have:     \n",
    "$$\n",
    "\\sigma_z^2 \\, = \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\sigma_x^2 \\, + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2  \\sigma_y^2  + 2  \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\sigma_{xy}\n",
    "$$\n",
    "- The **Monte-Carlo approach** can be used to estimate errors if the standard error propagation formula is suspected not to give reliable results (cf. 1st order truncation of the Taylor expansion). This method is particularly simple to implement in python as `scipy.stats.distribution.rvs()` allows one to draw samples from various univariate distributions. **Monte-Carlo** may also be used to create synthetic data sets that looks like your real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. References: <a class=\"anchor\" id=\"X\"></a>\n",
    "\n",
    "* **Chapter 1** (1.2), **Chapter 3** (3.1, 3.2) and **Chapter 4** (4.1 to 4.3) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "* *All of statistics: a concise course in statistical inference*, Wasserman 2004  <a class=\"anchor\" id=\"WAS04\"></a>(see also errata in http://www.stat.cmu.edu/~larry/all-of-statistics/).  \n",
    "\n",
    "* *Statistics in theory and Practice*, Lupton 1993 <a class=\"anchor\" id=\"LUP93\"></a>: **Chapter 2**\n",
    "\n",
    "* [Numerical recipes](http://www2.units.it/ipl/students_area/imm2/files/Numerical_Recipes.pdf) by Press et al. Cambridge University press: **Chapter 15**, **Chapter 18.7** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
