{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Statistical Inference: Uncertainty calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content:\n",
    "\n",
    "- II. What is statistical inference ? See [Frequentist_inference_01.ipynb](Frequentist_inference_01.ipynb)\n",
    "    * II.1 Point estimate.  See [Frequentist_inference_01.ipynb](Frequentist_inference_01.ipynb)\n",
    "    * II.2 Confidence interval. See [Frequentist_inference_02.ipynb](Frequentist_inference_02.ipynb)\n",
    "    * II.3 Hypothesis testing See [Frequentist_inference_H_test.ipynb](Frequentist_inference_H_test.ipynb)\n",
    "    * II.4 Uncertainty calculation using Monte-Carlo method:\n",
    "        - II.4.1 [Variable transformation](#II.4.1-Variable-transformation)\n",
    "            * II.4.1. [Log scale and the meaning of dex](#II.4.1.1-Base-10-logarithm-and-the-definition-of-the-\"dex\")\n",
    "        - II.4.2 [Error propagation formula](#II.4.2-Error-propagation-formula:)\n",
    "        - II.4.3 [Monte-Carlo-Error-estimate](#II.4.3-Monte-Carlo-Error-estimate:)\n",
    "    * II.5 Bootstrap. See [Frequentist_inference_Bootstrap.ipynb](Frequentist_inference_Bootstrap.ipynb)\n",
    "    \n",
    "- X. [References and supplementary material](#X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to be used in this notebook\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 Monte-Carlo methods and error estimates\n",
    "\n",
    "The random variable you are interested in is often a function of other random variables that are directly measured. A simple example in physics is the measurement of the speed of an object. You may not measure the speed directly but the distance and the time. For each time and distance you measure, you estimate have an associated uncertainty. But what you are interested in is the velocity, i.e. distance / unit of time. How do you propagate the two uncertainties to derive the uncertainty on the speed ? We'll see below how to proceed thanks to the **error propagation formula** (this is a formula you should know and apply !). But we'll also introduce a numerical approach (Monte-Carlo technique) to tackle that question. This is particularly useful if you are in a situation where you cannot easily apply the error propagation formula (you need to calculate partial derivatives on random variable), or when you suspect that this formula is not valid (it is an approximate formula).\n",
    "\n",
    "### II.4.1 Variable transformation\n",
    "\n",
    "Imagine that you have a sample of magnitude measurements (or relative magnitudes) of an object ( $m = -2.5 * \\log(F)$, where $F$ is the flux). How do you estimate the mean and uncertainty on the mean? You may be tempted to use the standard error on the mean (see [I.7.3](#I.7.3-Standard-error-on-the-mean) ), but you cannot because the magnitude is a derived quantity, the flux being what is effectively measured and for which you measure errors.    \n",
    "\n",
    "As you may know, any function of a random variable is a random variable. Let's consider the \"primal\" random variable $x$ and the function $y = \\phi(x)$ of the latter.    \n",
    "\n",
    "If we know the pdf $p(x)$ (where $x$ is a random variable), then [remember](../03-Basic_statistics_and_proba_concepts/Descriptive_statistics_01.ipynb) that the probability distribution $p(y)$, where $y = \\phi(x)$ (and so $x = \\phi^{-1}(y)$), is:\n",
    "\n",
    "$$\n",
    "p(y) = P'(y) = p\\left[ \\phi^{-1} \\left(y \\right) \\right] \\left| \\frac{{\\rm d}\\,\\phi^{-1}(y) }{{\\rm{d}} y} \\right|\n",
    "$$\n",
    "\n",
    "where $P'(y)$ is the first derivative of the CDF.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where $x$ is drawn from a uniform distribution, but we study $y = -2.5*\\log(x)$. Then we have $x = \\phi^{-1}(y) = 10^{-0.4*y} $. Since we have a uniform distribution for $x$, we have $p(x) = 1$ if $0 \\leq x \\leq 1$, and therefore $p(y) =  0.4\\, \\ln(10) \\, 10^{-0.4\\,y}$ with $0 \\,< \\,y \\,<\\,\\infty $. \n",
    "\n",
    "Note that we take the absolute value of the first derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a uniform distribution\n",
    "uniform_dist = scipy.stats.uniform(0.0001, 1)\n",
    "x_sample = uniform_dist.rvs(1000)\n",
    "# Calculate the pdf of x to overplot it\n",
    "x = np.linspace(-0.5, 1.5, 1000)\n",
    "Px = uniform_dist.pdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "y_sample = -2.5*np.log10(x_sample)\n",
    "# Calculate the pdf of y to overplot it\n",
    "y = -2.5*np.log10(x)\n",
    "Py = 0.4 * np.log(10) * Px * 10**(-0.4*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Uniform and transformed distribution \n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.hist(x_sample, 20, histtype='stepfilled', fc='blue', density=True)\n",
    "ax.plot(x, Px, '-k')\n",
    "ax.set_xlim(-0.2, 1.2)\n",
    "ax.set_ylim(0, 1.4001)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "ax.text(0.95, 0.95, r'$p_x(x) = {\\rm Uniform}(x)$', va='top', ha='right', transform=ax.transAxes)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$p_x(x)$')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.hist(y_sample, 20, histtype='stepfilled', fc='blue', density=True)\n",
    "ax.plot(y, Py, '-k')\n",
    "#ax.set_xlim(0.85, 2.9)\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "ax.text(0.95, 0.95, '$y=-2.5\\,\\log(x)$\\n$p_y(y)=p_x(c\\,10^{-0.4\\,y})$', va='top', ha='right', transform=ax.transAxes)\n",
    "ax.set_xlabel('$y$')\n",
    "ax.set_ylabel('$p_y(y)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.4.1.1 Base 10 logarithm and the definition of the \"dex\" \n",
    "\n",
    "Very frequently in astrophysics and geophysics, the quantity that we are interested in / that we are reporting is not the quantity itself, but its base 10 logarithm. This is for example the case of the mass of galaxies or galaxy halos ($\\log(M_{\\rm{Halo}}/M_{\\odot})$), supermassive black holes ($\\log(M_{BH}/M_{\\odot})$), the metallicity ($\\log[(Fe/H)/(Fe/H)_{\\odot}]$), ... Naturally, the reported uncertainty is also the uncertainty on the logarithm (in base 10) of the quantity. The variable transformation formula allow you to easily convert the error if you have been working with the quantity itself and not its log. In that framework, you will often read / hear scientist stating that the uncertainty on the mass of this BH is e.g. **0.3 dex**. What does **dex** mean? **One dex** simply means one order of magnitude. More formally, a difference of $x$ dex is a change by a factor of $10^x$. For our black hole mass example, an uncertainty of 0.3 dex corresponds approximately to a factor $10^{0.3} \\sim 2$ uncertainty. If the black hole mass is $10^9 M_{\\odot} \\pm 0.3$ dex whith 0.3 dex being the 1 standard error, it means that, if the uncertainties are gaussian, the 68.3\\% confidence interval is $[10^{8.7}, 10^{9.3}] M_{\\odot}$. Think of how you would have reported your uncertainties not using the \"dex\" \n",
    "\n",
    "It also means that if authors state that they assume a gaussian distribution centered on $\\log(M/M_{\\odot}) = \\mu = 12$ with a standard deviation of 0.3 dex, they used a $N(12,0.3)$. Beware however that if a quantity follows a normal distribution, it is unlikely that its $\\log$ will follow a normal distribution (you can check it out analytically or numerically using python - see [II.4.3.](#II.4.3-Monte-Carlo-Error-estimate:) to find out how to proceed). \n",
    "\n",
    "More generally the term dex is also used as a replacement for \"order of magnitude\". [Don't be mistaken with the definition of magnitude by astronomers](https://ui.adsabs.harvard.edu/abs/1960Natur.187..879H/abstract). An order of magnitude means **a factor 10**, two orders of magnitudes a factor of $10^2 = 100$, ... But again, the principal motivation for using dex is the ability to use report fractional uncertainties easily. A fractional uncertainty of 25% corresponds to $\\log(1.25) \\sim 0.1 $ dex. \n",
    "\n",
    "As a historical footnote, the term “dex” was coined by the astronomer C. W. Allen during an IAU meeting in 1948, and a [short note advocating its use](https://ui.adsabs.harvard.edu/abs/1951Obs....71..157A/abstract) has been published in 1951 (The Observatory, Vol. 71, p. 157-157).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.2 Error propagation formula:\n",
    "\n",
    "\n",
    "How do you proceed if you have a function of more than one random variable ? Then, you need to use the *error propagation formula* given below. This is a **very important** formula that you need to know. Knowing the demonstration is not compulsory, but understanding it is insightful and allows you to understand the assumptions entering into the formula.   \n",
    "\n",
    "If we have a function $z = \\phi(x,y)$, where $x $ and $y$ are random variable with sample values $x_i = {x_1, ..., x_n}$ and $y_i = {y_1, ..., y_n}$, then the error on $\\sigma_z$ is given by the **Error propagation formula**:  \n",
    "\n",
    "$$\n",
    "\\sigma_z^2 \\, = \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\sigma_x^2 \\, + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2  \\sigma_y^2  + 2  \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\sigma_{xy}\n",
    "$$\n",
    "\n",
    "*Demonstration:* \n",
    "\n",
    "We can define $z_i$ as $\\phi(x_i, y_i)$ (i.e. $\\phi$ evaluated at $(x_i, y_i)$), and $\\bar{z}$ as $\\phi(\\bar{x}, \\bar{y})$ (i.e. $\\phi$ evaluated at its mean value). \n",
    "\n",
    "The Taylor expansion of $z$ around its average value is:\n",
    "\n",
    "$$\n",
    "z_i = \\phi(\\bar{x}, \\bar{y}) \\, + \\,  \\left(\\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}  \\, (x_i-\\bar{x})+ \\, \\left (\\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\, (y_i-\\bar{y})  ~ + ~{\\rm {higher~order~terms}}\n",
    "$$\n",
    "\n",
    "If the measured values $x_i, y_i$ are close to the average, then, neglecting the higher order terms, we have:   \n",
    "\n",
    "$$\n",
    "z_i - \\bar{z} = \\left(\\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}  \\, (x_i-\\bar{x})+ \\, \\left (\\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\, (y_i-\\bar{y})\n",
    "$$\n",
    "\n",
    "We can then calculate the variance on $\\Phi$ which is:    \n",
    "\n",
    "$$\n",
    "\\sigma_z^2 \\, = \\, \\sum_i^{N} \\, \\frac{(z_i - \\bar{z})^2}{N} \n",
    "$$\n",
    "\n",
    "Hence,    \n",
    "$$\n",
    "\\sigma_z^2 \\, = \\, \\frac{1}{N} \\sum_i^{N} \\, (x_i-\\bar{x})^2 \\, \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\,   +  \\frac{1}{N} \\sum_i^{N} \\, (y_i-\\bar{y})^2 \\, \\left (\\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2 \\,  + \\frac{2}{N}  \\sum_i^{N}   \\, (x_i-\\bar{x}) (y_i-\\bar{y}) \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_z^2 \\, = \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\sigma_x^2 \\, + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2  \\sigma_y^2  + 2  \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\sigma_{xy}\n",
    "$$\n",
    "\n",
    "Such error estimates for non-linear functions (i.e. non linear in their variables) are biased on account of using a truncated series expansion. \n",
    "\n",
    "Note also that even in the simpler case where $z = \\phi(x)$ (simple transformation of variable), there is no guarantee, if $x$ follows a gaussian distribution of width $\\sigma_x$, that the transformed variable $z$ also follows a gaussian distribution of width $\\sigma_z$. \n",
    "\n",
    "**Conclusions:** Although there is no explicit assumption regarding the pdf in the derivation of the \"error propagation\" formula, there is an implicit one which comes from neglecting higher order terms. We'll see hereafter how Monte-Carlo simulations can be used to study the distribution of a transformed variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.3 Monte-Carlo Error estimate:\n",
    "\n",
    "The methods that consists in generating random points drawn from a specific distribution function and use them for various purposes are generically classified among *Monte-Carlo* techniques. \n",
    "\n",
    "By using Monte-Carlo simulations, we can verify the validity of the error propagation formula, but also have a way to derive the mean value and variance of a random variable that results from a combination of several random variables. Indeed, random samples allow us to approximate a pdf by drawing a virtually infinite samples from it. This means that:\n",
    "\n",
    "$$\n",
    "E(f(x)) \\, = \\, \\int\\limits_{-\\infty}^{+\\infty} f(x) h(x) dx \\rightarrow \\frac{1}{N}\\sum\\limits_i^N f(x_i),\n",
    "$$\n",
    "\n",
    "where $x_i$ is a random number drawn from the distribution $h(x)$, which, for the purpose of Monte-carlo simulations, is a uniform distribution ($h(x) = 1/(b-a)$).   \n",
    "If we specialize that formula for the mean, we have:\n",
    "\n",
    "$$\n",
    "\\mu = \\int_{-\\infty}^{+\\infty} x \\,h(x) dx \\rightarrow \\frac{1}{N}\\sum\\limits_i^N x_i,\n",
    "$$\n",
    "\n",
    "and the variance\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 \\, h(x) dx \\rightarrow \\frac{1}{N}\\sum\\limits_i^N (x_i - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "It's basically as if we did our experiment nearly infinitely many times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's first apply the Monte-carlo approach to the simple case of the sum of two random variables ($a$ and $b$) drawn from a gaussian distribution, and compare the result to the Gaussian distribution we derive from std error propagation formula:\n",
    "\n",
    "    * **z = a + b**\n",
    "\n",
    "*Standard error propagation:*\n",
    "\n",
    "$\\sigma_z^2 = \\left(\\frac{\\partial z}{\\partial a}\\right)^2 \\sigma_a^2 + \\left(\\frac{\\partial z}{\\partial b}\\right)^2 \\sigma_b^2$\n",
    "\n",
    "$\\Longrightarrow \\sigma_z^2 =  \\sigma_a^2 + \\sigma_b^2$\n",
    "\n",
    "The comparison between the two methods is given as an **Exercise**. Follow the different steps outlined in the next 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE \n",
    "# Define 2 random variable a drawn from N(mu_a, sigma_a)  and b drawn from N(mu_b, sigma_b)\n",
    "\n",
    "# We give values to mu_a, mu_b, sigma_a, sigma_b\n",
    "\n",
    "\n",
    "# Create an instance a of N(mu_a, sigma_a), and b of N(mu_b, sigma_b) using scipy.stats.norm()\n",
    "\n",
    "\n",
    "# Generate a random sample of 10000 points for each distribution \n",
    "\n",
    "\n",
    "# Our random variable z is the sum of the 2 random variables a and b \n",
    "# => we generate a sample of z which is the sum of sample_a and sample_b\n",
    "\n",
    "\n",
    "# Visualize the distribution of the Monte-Carlo sample you just created  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE - 2nd part\n",
    "# We want to compare the Monte-carlo to error propagation formula expectation \n",
    "# => we create, for comparison, z drawn from N(mu_z, sigma_z) with mu_z and sigma_z given by error propagation\n",
    "\n",
    "\n",
    "# Plot the distribution of your Montecarlo sample and overplot the PDF of the N(mu_z, sigma_z) distribution \n",
    "# with (mu_z, sigma_z) given by the error propagation formula \n",
    "\n",
    "\n",
    "# Calculate the estimate of the mean and std from the error propagation \n",
    "\n",
    "\n",
    "# Print the values of the estimate of the mean and std from the error propagation and those derived with MC\n",
    "# Do they agree ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's now try with the product of two random variables:  **z = a * b**\n",
    "\n",
    "Standard Error Propagation:\n",
    "\n",
    "$\\sigma_z^2 = b^2\\sigma_a^2 + a^2\\sigma_b^2 = z^2\\left(\\left(\\frac{\\sigma_a}{a}\\right)^2 + \\left(\\frac{\\sigma_b}{b}\\right)^2\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the 2 distributions for rv_1= a and rv_2 = b ; here we take 2 gaussian parent distributions\n",
    "mu_a, sigma_a = 1., 0.6\n",
    "mu_b, sigma_b = 1., 1.\n",
    "\n",
    "a = scipy.stats.norm(mu_a, sigma_a)\n",
    "b = scipy.stats.norm(mu_b, sigma_b)\n",
    "sample_a = a.rvs(10000)\n",
    "sample_b = b.rvs(10000)\n",
    "# let's build the random variable z = a * b\n",
    "sample_z = sample_a * sample_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build N(mu_z, sigma_z) where sigma_z is calculated from the error propagation formula\n",
    "mu_z = (mu_a * mu_b) \n",
    "sigma_z = mu_z * np.sqrt((sigma_a / mu_a)**2  + (sigma_b / mu_b)**2)  # error propagation formula\n",
    "z = scipy.stats.norm(mu_z, sigma_z)\n",
    "\n",
    "z_hist = plt.hist(sample_z, bins=50, normed=True, color='blue', alpha=0.3, label='MC')\n",
    "x = np.linspace(-5, 40, 100)\n",
    "plt.plot(x, z.pdf(x), color='red', label='Std err')\n",
    "plt.legend()\n",
    "\n",
    "sample_mu_z = np.mean(sample_z)\n",
    "sample_sig_z = np.std(sample_z)\n",
    "\n",
    "print(\"Error from error propagation: z = %1.2f +- %1.2f\" % (mu_z, sigma_z))\n",
    "print(\"MC Error: z = %1.2f +- %1.2f\" % (sample_mu_z, sample_sig_z) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the error on b in the above example, and see how the final distribution behaves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "We have seen (that): \n",
    "\n",
    "- How the distribution of a random variable $X$ is transformed the we consider a function $Y = \\phi(X)$. The change of variable formula is similar to what we use for non random variable.\n",
    "- The **\"dex\"** means **one order of magnitude (= a factor 10)**. It is conveniently used to report large fractional errors. It is useful to remember that 0.1 dex corresponds to 25\\% fractional error, and 0.3 dex to a change by a factor 2, and (by definition) 1 dex is a factor 10.   \n",
    "- Do not forget that in python, `np.log()` is the *natural* logarithm (or Neperian logarithm). The logarithm in base 10 is `np.log10()`!  \n",
    "- The **standard error propagation formula** allows us to calculate the uncertainty on a random variable $Z = \\phi(X, Y)$.  We have:     \n",
    "$$\n",
    "\\sigma_z^2 \\, = \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x}^2  \\sigma_x^2 \\, + \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y}^2  \\sigma_y^2  + 2  \\left( \\frac{\\partial \\phi}{\\partial x} \\right)_\\bar{x} \\left( \\frac{\\partial \\phi}{\\partial y} \\right)_\\bar{y} \\sigma_{xy}\n",
    "$$\n",
    "- The **Monte-Carlo approach** can be used to estimate errors if the standard error propagation formula is suspected not to give reliable results (cf. 1st order truncation of the Taylor expansion). This method is particularly simple to implement in python as `scipy.stats.distribution.rvs()` allows one to draw samples from various univariate distributions. **Monte-Carlo** may also be used to create synthetic data sets that looks like your real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. References: <a class=\"anchor\" id=\"X\"></a>\n",
    "\n",
    "* **Chapter 1** (1.2), **Chapter 3** (3.1, 3.2) and **Chapter 4** (4.1 to 4.3) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "* *All of statistics: a concise course in statistical inference*, Wasserman 2004  <a class=\"anchor\" id=\"WAS04\"></a>(see also errata in http://www.stat.cmu.edu/~larry/all-of-statistics/).  \n",
    "\n",
    "* *Statistics in theory and Practice*, Lupton 1993 <a class=\"anchor\" id=\"LUP93\"></a>: **Chapter 2**\n",
    "\n",
    "* [Numerical recipes](http://www2.units.it/ipl/students_area/imm2/files/Numerical_Recipes.pdf) by Press et al. Cambridge University press: **Chapter 15**, **Chapter 18.7** \n",
    "\n",
    "* About the meaning origin of \"dex\": this blog post allowed me to find the reference to Allen's note [https://joe-antognini.github.io/astronomy/what-is-a-dex](https://joe-antognini.github.io/astronomy/what-is-a-dex). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3_lectures]",
   "language": "python",
   "name": "conda-env-py3_lectures-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
