{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Statistical Inference: Bootstrap and Jacknife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "- II. What is statistical inference ? See [Frequentist_inference_01.ipynb](Frequentist_inference_01.ipynb)\n",
    "    * II.3 Hypothesis testing See [Frequentist_inference_H_test.ipynb](Frequentist_inference_H_test.ipynb)\n",
    "    * II.4 Uncertainty calculation using Monte-Carlo method. See [Frequentist_inference_Monte_Carlo.ipynb](Frequentist_inference_Monte_Carlo.ipynb)\n",
    "    * II.5 Bootstrap:\n",
    "       - [II.5.1 Bootstrap confidence interval](#II.5.1-Bootsrap-confidence-interval) \n",
    "       - [II.5.2 Jacknife](#II.5.2-Jacknife)     \n",
    "    \n",
    "- X. [References and supplementary material](#X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.5 Bootstrap\n",
    "\n",
    "The bootstrap is a method for estimating standard error (and confidence intervals on some statistical quantities / aka statistics) using your observed sample of random variable to estimate the population distribution of your statistics. \n",
    "\n",
    "Imagine that you have a random variable $x$, and have n measurements $\\{x_i\\}$ of it. Then you may want to measure some statistics (let's say $q = g(x_1, ..., x_n)$) of the pdf of $x$ (e.g. its mean, its median, its variance, ...) and find out how precise is that statistics (i.e. measure the variance or confidence interval associated to that stat.). In general, you do not know the distribution $h(x)$ from which are drawn your data, and therefore you do not know the one of $q$. You only have an estimate $\\hat{h}(x) \\, \\equiv\\, f(x)$:\n",
    "\n",
    "$$\n",
    "f(x) \\, = \\, \\frac{1}{n} \\, \\sum_{i=1}^n \\, \\delta(x - x_i),  \n",
    "$$\n",
    "\n",
    "where $\\delta(x)$ is the dirac function. This choice of $f(x)$ is also the one that maximizes the likelihood, i.e. $p(x_i  \\, | \\, f)$. Now that we have an estimator of $h(x)$ we can draw samples from $f(x)$ (as we did when we used Monte-Carlo to derive pdf for virtually infinite samples of random variables), and study the distribution of our statistical quantity (i.e. estimate its variance, confidence interval, ...). This is exactly what one does when using a Monte-Carlo simulations *except* that instead of drawing samples from a known pdf $h(x)$ we do it from our estimator of the pdf $f(x)$.    \n",
    "\n",
    "If our sample contains $n$ elements, then, there are $n!$ possible distinct samples of size n we can draw , and a probability $n!/n^n$ that we draw a sample identical to the original data (i.e. for 10 elements, $n!/n^n \\sim 3.6\\,10^{-4}$.).  \n",
    "\n",
    "In practice:\n",
    "- You have: $x=\\{x_1, x_2, ..., x_n\\}$\n",
    "- You then draw $B$ samples $\\{x^*_1, x^*_2, ..., x^*_n\\}$, where $x^*_i$ is randomly picked among the $x$ (with replacements, i.e. it could be that one of your $B$ samples will be $\\{x_1, x_1, ...,x_1\\}$ !). In other words, for each bootstrap sample, you generate random n indices in the range $[1, ..., n]$  (in `python`, you can use `np.random.randint(low, high, size)` or `scipy.random.randint()` or `np.random.choice(mysample, size)` to generate n random integers) to build your bootstrap sample from the original sample. \n",
    "- Then, as we did with Monte-Carlo simulations, you can estimate from each of these $B$ samples your statistics $\\hat{q}$ and study its distribution, or more specifically derive its variance, confidence interval, ... If the statistics you derive from bootstrap is e.g. the mean of the population distribution, and if $q_B = {q^*_1, q^*_2, ..., q^*_{nboot}}$ is the ensemble of means derived from bootstrap, then the standard deviation of $q_B$ (i.e. $\\sigma = \\sqrt{\\sum (q^*_i - \\bar{q_B})^2/(n_{boot}-1)}$) yields a bootstrap estimate of the standard error on the mean. Indeed, the bootstrap allows you to derive the destribution of your statistics. \n",
    "\n",
    "\n",
    "**Note:** Sometimes, instead of using $f(x)$ as derived directly from the sample, one uses its \"best fit model\" $f(x \\, | \\, \\hat{\\theta})$, this is then called *parametric bootstrap*.  \n",
    "There is complete books dedicated to bootstraps and its subtleties (especially if the errors on your random variable are not identical, effects related to the size of your sample, ... ). \n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "- Create a random sample $x$ of $N=100$ measurements drawn from a normal gaussian distribution $N(0,4)$. This is assumed to be your data set. \n",
    "- Calculate a bootstrap estimate of the mean and the standard error on the mean. Compare the value of the standard error on the mean you expect for a sample of 100 points,  (i.e. $\\sigma/\\sqrt(n-1)$) to the value derived with the bootstrap. Consider a bootstrap sample of $n_{boot} = $ 1000. \n",
    "- Calculate an estimator of the std $s = \\hat{\\sigma}$ of your distribution and its associated (standard) error. \n",
    "- How do those values compare to the expected distribution (based on the fact that the population distribution of x is N(0,4) ? *TIP*: the Standard error on $s$ is $ stde(s) = s / \\sqrt{2(npts-1)}$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules \n",
    "import numpy as np\n",
    "import scipy.stats \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your random sample\n",
    "mu, sigma, npts = 0, 4, 100\n",
    "x_sample = scipy.stats.norm.rvs(loc=mu, scale=sigma, size=npts, random_state=12456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap mean = -0.206, Bootstrap std = 4.454 \n",
      "Population mean = 0.00, and std (gauss) = 4.00 \n",
      "Sample mean (sample) = -0.22, and std = 4.50 \n",
      "Bootstrap vs expected error on the mean vs sample stde 0.450 vs 0.402 vs 0.452\n",
      "-----------------------------------------------------------\n",
      "Std of the Bootstrap distribution = 4.45, and bootstrap stde(s) 0.30 \n",
      "Bootstrap vs expected error on the **std** 0.30 vs 0.28 \n"
     ]
    }
   ],
   "source": [
    "nboot = 1000 # Size of the bootstrap sample\n",
    "# Create an array where you will save the mean and variance of your bootstraped samples \n",
    "mean_boot, var_boot = np.zeros(nboot), np.zeros(nboot)\n",
    "# Use a for loop to generate nboot bootstrap samples and calculate their mean and variance\n",
    "np.random.seed(123485)\n",
    "for i in range(nboot):\n",
    "    bootsamp = np.random.choice(x_sample, size=npts, replace = True)\n",
    "    mean_boot[i], var_boot[i] = bootsamp.mean(), bootsamp.var(ddof=1)\n",
    "\n",
    "# Bootstrap estimate of the mean \n",
    "mu_boot = mean_boot.mean()\n",
    "# Bootstrap estimate of the stde on the mean \n",
    "stde_mu = mean_boot.std(ddof = 1)\n",
    "# You can also calculate it explictly using the stdev definition\n",
    "stde_mu_from_boot = np.sqrt(np.sum( (mean_boot - mu_boot)**2 / (nboot-1)) ) \n",
    "\n",
    "# \"Population\" based Stde on the mean (unknown in real cases as we would not know sigma)\n",
    "stde_population = sigma / np.sqrt(npts - 1)\n",
    "\n",
    "# Sample mean and stde on the mean (that you would have estimated from your original sample)\n",
    "sample_mean = x_sample.mean()\n",
    "sample_std = x_sample.std(ddof=1)\n",
    "sample_stde = x_sample.std(ddof=1) / np.sqrt(npts - 1)\n",
    "\n",
    "# Print the values of the mean, stde, s, stde(s)\n",
    "std_boot = np.sqrt(var_boot)   # Bootstrap Estimates of the std of the population (1000 values)\n",
    "err_std = sigma / np.sqrt(2. * (npts -1))\n",
    "\n",
    "print('Bootstrap mean = %.3f, Bootstrap std = %.3f ' % (mean_boot.mean(), std_boot.mean() )  )\n",
    "print('Population mean = %.2f, and std (gauss) = %.2f ' %(mu, sigma) )\n",
    "print('Sample mean (sample) = %.2f, and std = %.2f ' %(sample_mean, sample_std) )\n",
    "print('Bootstrap vs expected error on the mean vs sample stde %.3f vs %.3f vs %.3f' %(stde_mu_from_boot, stde_population, sample_stde ) )\n",
    "print('-----------------------------------------------------------' )\n",
    "print('Std of the Bootstrap distribution = %.2f, and bootstrap stde(s) %.2f ' % (std_boot.mean(), std_boot.std(ddof=1) ) )\n",
    "print('Bootstrap vs expected error on the **std** %.2f vs %.2f ' % (std_boot.std(ddof=1), err_std ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap does not add much when you know the original distribution of your random variable, but if you do not, then it can be very helpful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Bootstrap dist. for the std')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(ncols=2, figsize=(8,4))\n",
    "ax[0].hist(mean_boot, bins=50);\n",
    "ax[1].hist(np.sqrt(var_boot), bins=50);\n",
    "ax[0].set_title('Bootstrap dist. for the mean')\n",
    "ax[1].set_title('Bootstrap dist. for the std')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "In recent versions of the `astropy` module, there is an implementation of the bootstrap algorithm: \n",
    "```python \n",
    "from astropy.stats import bootstrap\n",
    "bootsamp = bootstrap(x_sample, nboot)\n",
    "mean_boot_apy = bootsamp.mean(axis=1)\n",
    "std_boot_apy =  bootsamp.std(ddof=1, axis=1)\n",
    "print('Bootstrap mean = %.2f , std= %.2f ' % (mean_boot_apy.mean(), std_boot_apy.mean() ))\n",
    "```\n",
    "You may *NOT* use it to solve exercises given in this lecture or at the exam (unless this is for checking your results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap mean = -0.206 , std= 4.454 \n"
     ]
    }
   ],
   "source": [
    "from astropy.stats import bootstrap\n",
    "np.random.seed(123485)\n",
    "bootsamp = bootstrap(x_sample, nboot)\n",
    "mean_boot_apy = bootsamp.mean(axis=1)\n",
    "std_boot_apy =  bootsamp.std(ddof=1, axis=1)\n",
    "print('Bootstrap mean = %.3f , std= %.3f ' % (mean_boot_apy.mean(), std_boot_apy.mean() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.5.1 Bootsrap confidence interval \n",
    "\n",
    "If we calculate a statistics $q$\n",
    "There are many ways to derive confidence intervals using bootstrap: \n",
    "\n",
    "* -1.- **Normal Interval**: \n",
    "If you assume that your statistics follows a normal distribution (the interval is only accurate if this is true), \n",
    "$$\n",
    "[\\hat{q} - z_{\\alpha/2} \\, stde^B(q), \\hat{q}+z_{\\alpha2}\\, stde^B (q)],\n",
    "$$ \n",
    "\n",
    "where $\\hat{\\theta}$ is the estimator based on the original distribution, and $stde^B$ is the bootstrap standard error. \n",
    "\n",
    "Similarly, one can also use the student's distribution to estimate the confidence interval, hence replacing $z_{\\alpha2}$ by $t_{\\alpha/2}$ \n",
    "\n",
    "\n",
    "For a 95% normal confidence interval, you have:\n",
    "$$\n",
    "[\\hat{q} - 1.96 \\, stde^B(q), \\hat{q} + 1.96 \\, stde^B (q)],\n",
    "$$\n",
    "\n",
    "* -2.- **Percentile**: You simply take the $\\alpha/2$, $1-\\alpha/2$ percentiles of the distribution of your $q^*_i$:    \n",
    "\n",
    "$$\n",
    "[q^*_{\\alpha/2}, q^*_{1-\\alpha/2}]\n",
    "$$\n",
    "\n",
    "If the bootstrap replicates of $\\hat{q}^{*}$ follow a normal distribution $N(\\hat{q}, stde^B (q))$, this percentile interval works reasonably well in many situations. It has also the property to be invariant to monotone transformations of $q$ (i.e. $m(q)$). \n",
    "\n",
    "In `python`, if you have an array of values (such as the array containing your $q^*_i$), you can use the numpy function `np.percentile(array, p)` where $p$ is the p-percentile value. If you which to calculate 68% CI, then you will chose `p = [34./2, 100-34./2]`. \n",
    "\n",
    "* -3.- A third approach is the so-called \"pivotal confidence intervals\". Those are defined as:\n",
    "\n",
    "$$\n",
    "[2 \\hat{q} - q^*_{1-\\alpha/2}, 2 \\hat{q} - q^*_{\\alpha/2})],\n",
    "$$\n",
    "\n",
    "* Other approaches exist in the litterature. There is a little documented module [scikits.bootstrap](https://pypi.org/project/scikits.bootstrap/#description) for bootstrap confidence interval in python. Hence, you may need to develop your own functions for estimating bootstrap confidence interval. \n",
    "\n",
    "**Exercise**:\n",
    "\n",
    "Calculate a 95% CI (i.e. the normal interval, the percentile interval and the pivotal interval) for the sample used in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our mean mu_hat =  -0.218  \n",
      "Normal CI [-1.101 , 0.664]\n",
      "Percentile CI : [-1.094, 0.623 ]\n",
      "Pivot CI : [-1.060, 0.658]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05 \n",
    "zalpha = 1.96 #(for a 95% CI)\n",
    "\n",
    "hat_q = x_sample.mean()\n",
    "print('Our mean mu_hat =  %.3f  ' % hat_q )\n",
    "\n",
    "CIlow, CIhigh = x_sample.mean() - zalpha * stde_mu_from_boot, x_sample.mean() + zalpha * stde_mu_from_boot\n",
    "print('Normal CI [%.3f , %.3f]' % (CIlow, CIhigh) )\n",
    "\n",
    "# Percentile CI\n",
    "ca1, ca2 = np.percentile(mean_boot, [2.5, 97.5])\n",
    "print('Percentile CI : [%.3f, %.3f ]' %(ca1, ca2) )\n",
    "\n",
    "# Pivot CI\n",
    "print('Pivot CI : [%.3f, %.3f]' %(2*hat_q - ca2, 2*hat_q - ca1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.5.2 Jacknife\n",
    "\n",
    "The jacknife is a sort of bootstrap but in principle more robust (that claimed robustness gave it its name). The idea of the jacknife is to leave one observation aside for computing the statistic of interest. \n",
    "\n",
    "If $q_n$ is a statistical quantity estimated based on a sample of $n$ points. If we remove one data point, we can calculate $n$ times the statistics $q^*_i$ based on $n-1$ points (each time removing a *different* point).  \n",
    "$q_{\\infty}$ is the statistics we would have for an infinite sample. It can be shown (expanding $q_n$ as a series in $n^{-1}$, i.e. $q_n = q_{\\infty} + n^{-1}\\, q'_n+O(n^{-2})$ ; see [Lupton 1993](#LUP93)) that a bias corrected estimate $q^J$ can be computed as:\n",
    "\n",
    "$$\n",
    "q^J = q_n \\, + \\, \\Delta\\,q\n",
    "$$\n",
    "\n",
    "where $q_n$ is the estimator derived from the full sample and the jacknife correction $\\Delta\\,q$ is:\n",
    "\n",
    "$$\n",
    "\\Delta \\, q = (n-1)\\, \\left( q_n \\, - \\, \\frac{1}{n}\\, \\sum_{i=1}^n \\, q^*_i \\right)\n",
    "$$\n",
    "\n",
    "Therefore, the bias-corrected estimate of q is simply:\n",
    "\n",
    "$$\n",
    "q^J = n \\, q_n \\, - \\, (n-1) \\, \\bar{q}_n \n",
    "$$\n",
    "\n",
    "\n",
    "In addition, if the estimator $q_n$ is asymptotically normal, then the variance for the jacknife estimate is:\n",
    "\n",
    "$$\n",
    "\\sigma^2_q \\, = \\, \\frac{n-1}{n} \\sum^n_{i=1} (q^*_i \\, - \\, \\bar{q}_n )^2\n",
    "$$\n",
    "\n",
    "with $\\bar{q}_n = n^{-1} \\, \\sum^n_{i=1} \\, q^*_i$  ($q^*_i$ is the stat where we have removed the $i$-th sample). The standard error $\\hat{se}^J$ on the jacknife statisitcs $q^J$ is simply the sqrt of the variance $\\sigma^2_q$.  \n",
    "\n",
    "**Exercise**\n",
    "\n",
    "For the same distribution as the one used in the bootstrap case, calculate the mean and standard error on the mean and compare to the bootstrap result.    \n",
    "TIP: You can use `sample = np.delete(sample, index)` to remove `sample[index]` from sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacknife Mean = -0.218, stde= 0.450\n",
      "Bootstrap mean = -0.220, stde 0.442 \n",
      "Sample mean and stde = -0.218,  0.450 \n",
      "Jacknife IQ = 5.754 \n",
      "Bootstrap IQ = 5.916 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1236)\n",
    "\n",
    "# let's redo the bootstrap as well for comparison\n",
    "nboot = 1000\n",
    "mean_boot = np.zeros(nboot)\n",
    "IQ_boot = np.zeros(nboot)  # I also add the interquartile\n",
    "for i in range(nboot):\n",
    "    bootsamp = np.random.choice(x_sample, size=npts, replace = True)\n",
    "    mean_boot[i] = bootsamp.mean()\n",
    "    IQ_boot[i] = np.percentile(bootsamp, 75) - np.percentile(bootsamp, 25)   \n",
    "\n",
    "# stde on the mean from the bootstrap \n",
    "mu_boot = mean_boot.mean()\n",
    "# stde_mu_boot = np.sqrt(np.sum( (mean_boot - mu_boot)**2 / (nboot-1)) )  # Explicit formula for the stde\n",
    "stde_mu_boot = mean_boot.std(ddof=1)    \n",
    "\n",
    "# Jacknife\n",
    "njack = npts  # njack = number of time to resample ; maximum is npts = Number of points of the sample ! \n",
    "mean_jack = np.zeros(njack)\n",
    "IQ_jack = np.zeros(njack)\n",
    "\n",
    "for i in range(njack):\n",
    "    jacksamp = x_sample.copy()\n",
    "    jacksamp = np.delete(jacksamp, i)\n",
    "    mean_jack[i] = np.mean(jacksamp)\n",
    "    IQ_jack[i] = np.percentile(jacksamp, 75) - np.percentile(jacksamp, 25)\n",
    "\n",
    "IQ_samp = np.percentile(x_sample, 75) - np.percentile(x_sample, 25)\n",
    "xbar_jack = npts * np.mean(x_sample) - (npts-1) * np.mean(mean_jack) # Bias corrected value of the jacknife estimate\n",
    "var_jack = ((npts-1) / npts) * np.sum( (mean_jack-xbar_jack)**2 )\n",
    "stde_jack = np.sqrt(var_jack)\n",
    "\n",
    "IQ_jack = npts * IQ_samp - (npts-1) * np.mean(IQ_jack)   # Apply a bias correction to the IQ of the jacknife\n",
    "\n",
    "print('Jacknife Mean = %.3f, stde= %.3f'% (xbar_jack, stde_jack ) )\n",
    "print('Bootstrap mean = %.3f, stde %.3f ' % (mean_boot.mean(), stde_mu_boot )  )\n",
    "print('Sample mean and stde = %.3f,  %.3f ' %(np.mean(x_sample), np.std(x_sample, ddof=1)/np.sqrt(npts) ) )\n",
    "print('Jacknife IQ = %.3f '% IQ_jack )\n",
    "print('Bootstrap IQ = %.3f '% np.mean(IQ_boot) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IQ =  5.395918001568654\n"
     ]
    }
   ],
   "source": [
    "pop_norm = scipy.stats.norm(0, 4)\n",
    "i25, i75 = pop_norm.ppf((0.25, 0.75))\n",
    "IQ = i75-i25\n",
    "print('IQ = ', IQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "In recent versions of the [astropy](https://docs.astropy.org/en/stable/api/astropy.stats.jackknife_stats.html) module, there is an implementation of the jacknife resampling and statistics: \n",
    "```python \n",
    "import astropy.stats \n",
    "# Create a jacknife resample\n",
    "jack_sample = astropy.stats.jackknife_resampling(x_sample)\n",
    "# jack_sample is an array of shape=(len(x_sample), len(x_sample) - 1 )\n",
    "\n",
    "# Calculate a statistic of interest, including a CI at 95% confidence \n",
    "test_statistic = np.mean\n",
    "estimate, bias, stderr, conf_interval = astropy.stats.jackknife_stats(x_sample, test_statistic, confidence_level=0.95)\n",
    "print(estimate, bias, stderr, conf_interval)\n",
    "```\n",
    "You may *NOT* use it to solve exercises given in this lecture or at the exam (unless this is for checking your results). \n",
    "\n",
    "#### Note: Bayesian Jacknife \n",
    "\n",
    "A paper by Wilensky et al. (2022, [https://arxiv.org/abs/2210.17351](https://arxiv.org/abs/2210.17351) proposes a Bayesian jackknife test for assessing the probability that a data set contains biased subsets. It also enables one to identify which of the subsets is/are likely to be biased. The test can be used to assess the presence and likely source of statistical tension between different measurements of the same quantities in an automated manner. Under certain broadly applicable assumptions, the test is analytically tractable. The team provides an open source code, [CHIBORG](https://github.com/mwilensky768/chiborg), that performs both analytic and numerical computations of the test on general Gaussian-distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.218150828526127 5.495603971894525e-15 0.45005685990425637 [-1.10024606  0.66394441]\n"
     ]
    }
   ],
   "source": [
    "import astropy.stats \n",
    "# Create a jacknife resample\n",
    "jack_sample = astropy.stats.jackknife_resampling(x_sample)\n",
    "# jack_sample is an array of shape=(len(x_sample), len(x_sample) - 1 )\n",
    "\n",
    "# Calculate a statistic of interest, including a CI at 95% confidence \n",
    "test_statistic = np.mean\n",
    "estimate, bias, stderr, conf_interval = astropy.stats.jackknife_stats(x_sample, test_statistic, confidence_level=0.95)\n",
    "print(estimate, bias, stderr, conf_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- You have learned 2 different techniques to calculate standard errors and confidence intervals for a statistics of a sample: the `bootstrap` and the `jacknife`. \n",
    "- The key concept of the bootstrap is that you generate fake samples (called bootstrap samples) based on the sample in hands (randomly picking elements from the original sample, *allowing for repetion*), and study the distribution of the statistic of interest based on this bootstrap sample. \n",
    "- The key concept of the jacknife consists in creating fakes samples (called jacknife samples), each jacknife being sample being the same as the original one, but with one point removed. The statistics of interest can be derived from this ensemble of jacknife samples (the mean and variance of the sample are derived using specific formula given in [II.5.2-Jacknife](#II.5.2-Jacknife)). \n",
    "- Since this method yields a distribution of the statistics of interest, it is possible to calculate a confidence interval based on the bootstrap of on the jacknife distributions. Bootstrap/Jacknife are particularly useful when the central limit theorem does not apply, or the statistics of interest is not the mean. \n",
    "- Three ways to calculate a confidence interval from a bootstrap sample of a specific statistics have been presented: the \"normal interval\" (assuming the distribution is gaussian), the \"percentile interval\", and the \"Pivotal confidence interval\".  \n",
    "- We can use the function `np.percentile()` to calculate a confidence interval for an array containing a statistic of interest (e.g. a bootstraped mean; or any array containing multiple realisation of a statistic of interest). \n",
    "- The maximum size of the bootstrap sample is given by $n!/n^n$, where $n$ is the number of points of the sample. \n",
    "- The package `astropy.stats` provides an implementation of the bootstrap and of the jacknife resampling. Those implementations can be used in real life but not to solve the problems given in this lecture (or at the exam). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. References and supplementary material: <a class=\"anchor\" id=\"X\"></a>\n",
    "\n",
    "**Chapter 4** (4.1 4.2, 4.5, 4.7) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "*All of statistics: a concise course in statistical inference*, Wasserman 2004  <a class=\"anchor\" id=\"WAS04\"></a>(see also errata in http://www.stat.cmu.edu/~larry/all-of-statistics/): **Chapter 8, 9**\n",
    "\n",
    "* *Statistics in theory and Practice*, Lupton 1993 <a class=\"anchor\" id=\"LUP93\"></a>: **Chapter 6, 7, 8, 9**\n",
    "\n",
    "* [Numerical recipes](http://www2.units.it/ipl/students_area/imm2/files/Numerical_Recipes.pdf) by Press et al. Cambridge University press: **Chapter 14**\n",
    "\n",
    "* A good recap of the concepts that we have seen in the previous lectures:  https://towardsdatascience.com/https-medium-com-aparnack-what-can-a-small-sample-teach-us-about-a-big-population-part-1-b7c048c22bf2 \n",
    "\n",
    "Other useful references to know more about the topics covered in this lecture: \n",
    "\n",
    "- Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/). Project Leader: David M. Lane, Rice University.\n",
    "\n",
    "- Bootstrap: A. C. Davison, D. V. Hinkley and G. A. Young, Statistical science, 2003, 18, 2, 141: Recent Developments in Bootstrap Methodology https://projecteuclid.org/journals/statistical-science/volume-18/issue-2/Recent-Developments-in-Bootstrap-Methodology/10.1214/ss/1063994969.full \n",
    "    \n",
    "- Fisher matrices and confidence ellipses (Not covered in this lecture): Coe 2009 https://arxiv.org/abs/0906.4123    \n",
    "\n",
    "- Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/). Project Leader: David M. Lane, Rice University.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
