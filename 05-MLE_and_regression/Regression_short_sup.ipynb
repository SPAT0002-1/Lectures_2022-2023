{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: supplementary material\n",
    "\n",
    "This Notebook contains some additional theory complementing the one covered in [Regression_short.ipynb](Regression_short.ipynb)\n",
    "\n",
    "## IV. Regression for linear models\n",
    "\n",
    "### IV.1 Regression for linear models\n",
    "\n",
    "#### IV.1.1 Fitting a straight line\n",
    "\n",
    "#### Heteroscedastic error and matrix formalism\n",
    "\n",
    "For heteroscedastic errors, and even more general regression function, one rather uses a more compact matrix notation.\n",
    "\n",
    "Our problem consists in finding a solution for: \n",
    "$$\n",
    "y_0 = \\theta_0 + \\theta_1 \\, x_0 + \\epsilon_0 \\\\\n",
    "y_1 = \\theta_0 + \\theta_1 \\, x_1 + \\epsilon_1 \\\\\n",
    "... \\\\\n",
    "y_{N-1} = \\theta_0 + \\theta_1 \\, x_{N-1} + \\epsilon_{N-1}\n",
    "$$\n",
    "\n",
    "We can therefore define $M$ (called design matrix) such that $Y = M\\,\\boldsymbol{\\theta} + E$, where $Y$ is a $N$ dimensional vector containing our $y_i$ (i.e. our $N$ points $y_i$):\n",
    "\n",
    "$$\n",
    "Y =  \\left[ \\begin{array}{c}\n",
    "y_0 \\\\ y_1  \\\\ ... \\\\ y_{N-1} \n",
    " \\end{array} \\right]   \n",
    "$$\n",
    "\n",
    "For our straight line regression, $\\boldsymbol{\\theta}$ is a vector containing our 2 parameters:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} =  \\left[ \\begin{array}{c}\n",
    "\\theta_0 \\\\ \\theta_1  \n",
    " \\end{array} \\right] ,\n",
    "$$\n",
    "\n",
    "$M$ is a $2 \\times N$ matrix:\n",
    "\n",
    "$$\n",
    "M =  \\left[ \\begin{array}{cc}\n",
    "1 & x_0 \\\\ 1 & x_1  \\\\ ... \\\\ 1 & x_{N-1} \n",
    " \\end{array} \\right], \n",
    "$$\n",
    "\n",
    "where the constant values in the first columns correspond to the constant value $\\theta_0$ in our regression. \n",
    "\n",
    "And finally, $E$ is \n",
    "$$\n",
    "E =  \\left[ \\begin{array}{c}\n",
    "\\epsilon_0 \\\\ \\epsilon_1  \\\\ ... \\\\ \\epsilon_{N-1}  \n",
    " \\end{array} \\right]   \n",
    "$$\n",
    "\n",
    "The $\\epsilon_i$ are distributed as $N(0, \\sigma_i)$, such that the associated $N\\times N$ covariance matrix $C$ is:\n",
    "\n",
    "$$\n",
    "C =  \n",
    " \\left[ \\begin{array}{cccc}\n",
    "\\sigma_0^2 & 0 & ... & 0 \\\\ \n",
    "0 & \\sigma_1^1 & ... & 0 \\\\ \n",
    "... & ... & ... & ... \\\\\n",
    "0 & 0 & ... & \\sigma_{N-1}^2   \n",
    " \\end{array} \\right] \n",
    "$$\n",
    "\n",
    "Note that the covariance matrix can have non-zero cross terms if there are the errors between data points is correlated, in which case, for $i \\neq l$,  $C_{il} = \\sigma_{il} \\neq 0 $\n",
    "\n",
    "Then the $\\chi^2$, i.e. the weighted sum of the square errors \n",
    "$$\n",
    "\\chi^2 \\equiv \\sum_i \\frac{(y_i - (\\theta_0+\\theta_1\\,x_i))^2}{\\sigma_i^2}, \n",
    "$$\n",
    "gets written with matrix representation: \n",
    "$$\n",
    "S = (Y - \\boldsymbol{\\theta} M)^T C^{-1} (Y-\\boldsymbol{\\theta} M).\n",
    "$$ \n",
    "\n",
    "Similarly to the homoscedastic case, the maximum likelihood solution for this regression is:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = (M^T C^{-1} M)^{-1} \\, (M^T C^{-1} Y),\n",
    "$$\n",
    "\n",
    "The uncertainties on the regression coefficients $\\boldsymbol{\\theta}$ are then expressed as the symmetric (covariance) matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma_\\theta = \\left[ \\begin{array}{cc}\n",
    "\\sigma_{\\theta_0}^2 & \\sigma_{\\theta_0 \\theta_1}  \\\\ \n",
    "\\sigma_{\\theta_0 \\theta_1} & \\sigma_{\\theta_1}^2 \\\\ \n",
    " \\end{array} \\right] = [M^T C^{-1} M]^{-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2 Regularization and penalization of the likelihood\n",
    "\n",
    "It is sometimes desirable to reduce the complexity of the regression model, for example when the likelihood has a complex structure, or when the data are highly correlated such that the matrices become ill conditioned and inversion impossible, or simply when some regions of the parameter space need to be avoided. In such cases, one apply a penalty to the likelihood function. \n",
    "\n",
    "Penalizing the regression is equivalent to minimizing \n",
    "$$\n",
    "\\chi^2 \\equiv \\sum_{i=1}^{n} \\frac{\\left(y_i - \\theta_0 - \\sum_{k=1}^p \\theta_p B_p(x_{i}) \\right)^2}{\\sigma^2_i},\n",
    "$$\n",
    "\n",
    "BUT adding a constraint on the parameters $\\theta_p$, such that e.g. $\\sum |\\theta_p | < s$ (LASSO regularisation) or $\\sum \\theta^2_p < s$ (Ridge regularization). \n",
    "\n",
    "\n",
    "Using the matrix formalism, this can be expressed in the following way:    \n",
    "We have seen that the ML minimization consists in minimizing:\n",
    "$$\n",
    "(Y - M \\boldsymbol{\\theta})^T (Y - M\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "The idea is to impose a penalty to this minimization term, namely:\n",
    "$$\n",
    "(Y - M \\boldsymbol{\\theta})^T (Y - M\\boldsymbol{\\theta}) + \\lambda\\,\\left |\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\right|,\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularisation coefficient, and $\\left |\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\right|$ is an example of penalty function. \n",
    "\n",
    "Solving for $\\boldsymbol{\\theta}$ ($\\partial / \\partial\\,\\theta_j = 0$), we end up:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = (M^T C^{-1} M + \\lambda \\,I)^{-1} \\, (M^T C^{-1} Y),\n",
    "$$\n",
    "where $I$ is the identity matrix. \n",
    "\n",
    "Interestingly, if $M^T C^{-1} M$ was singular (precluding derivation of any solution) this won't be the case for $(M^T C^{-1} M + \\lambda \\,I)$, such that regularization can yield more robustly to a solution. \n",
    "\n",
    "#### IV.2.1 Ridge regression:\n",
    "\n",
    "The use of $\\left|\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\right|$ for the regularisation is called \"ridge regression\" or \"Tikhonov regularisation\". It penalizes the sum of the square of the regression coefficients such that $|\\theta|^2 < s$. By precluding the sum of the squares of the coefficients to be too large. The smaller is the value of $s$ (which correspond to large $\\lambda$) the more the regression coefficients will be driven towards 0. \n",
    "\n",
    "Ridge regression is implemented in `sklearn.linear_model.Ridge`\n",
    "\n",
    "#### IV.2.2 LASSO regression:\n",
    "\n",
    "The LASSO (Least Absolute Schrinkage and Selection) penalization uses the absolute values of the coefficients $\\boldsymbol{\\theta}$ instead of their square (cf. `Ridge` regression) to penalize the likelihood as:\n",
    "\n",
    "$$\n",
    "(Y - M \\boldsymbol{\\theta})^T (Y - M\\boldsymbol{\\theta}) + \\lambda\\,\\left |\\boldsymbol{\\theta} \\right|,\n",
    "$$\n",
    "\n",
    "which is equivalent in least square regression to a penalty on the absolute value of the regression coefficients $|\\theta| < s$.  \n",
    "\n",
    "\n",
    "The disadvantage of LASSO regression is that there is no closed-form solution and numerical techniques need to be developed to find a solution. \n",
    "\n",
    "#### IV.2.3 How to fix $\\lambda$ ?:\n",
    "\n",
    "One can evaluate the impact the amplitude of $\\lambda$ on the regression by dividing the data set in subsets (typically, a *training set*, a *cross validation* set, and a *test set*) and evaluate how it modifies the fit. The value of $\\lambda$ that introduces the minimum error is probably the best one. \n",
    "\n",
    "This method, called *k-fold cross validation* can be summarized as follow:\n",
    "You split your data set in three parts: \n",
    "- A *training* set is used to evaluate the optimal values of $\\theta_i$ and the associated errors. \n",
    "- A *cross validation* set, that allows you to evaluate cross-validation errors. Because the cross validation was not used to evaluate the parameters, the errors may be larger in case of high bias in the fit. Therefore, the model representing the cross validation is likely to be in practice a better model. \n",
    "- The same procedure is then applied to the *test set*, giving an idea of the error we would make with a new data set. \n",
    "\n",
    "This approach, called \"k-fold cross validation\" is very common in machine learning studies, but can also be used to evaluate $\\lambda$ more objectively (provided that your data set is large enough to make it possible). Sect. 8.3.3 and 8.11.1 of our [book](#book) address those questions in slightly more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python tools to manage regularization and penalization of the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Package | Comments  | \n",
    "|-------|---------|--------|\n",
    "|Linear + regularization | `sklearn.linear_model.Ridge()` | $\\sum \\theta_p^2 < s$; `RidgeCV()` implements cross validation for getting best $\\lambda$|\n",
    "|Linear + regularization | `sklearn.linear_model.Lasso()` | $\\sum |\\theta_p| < s$; `LassoCV()` implements cross validation for getting best $\\lambda$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel-python3]",
   "language": "python",
   "name": "conda-env-intel-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
